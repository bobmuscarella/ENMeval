---
title: "ENMeval v1.0.0 Vignette"
author: "Jamie M. Kass, Robert Muscarella, and Peter Galante"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{ENMeval v1.0.0 Vignette}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r setup, echo = FALSE, include=FALSE}
library(knitr)
knitr::opts_chunk$set(collapse=TRUE, message=FALSE, warning=FALSE, comment="#>")
```

- [Introduction](#intro)
- [Data Acquisition & Pre-processing](#data)
- [Partitioning Occurrences for Evaluation](#partition)
- [Running ENMeval](#eval)
- [Plotting results](#plot)
- [Downstream Analyses](#downstream)
- [Resources](#resources)


## Introduction {#intro}

[`ENMeval`](https://cran.r-project.org/package=ENMeval) is an R package that performs automated runs and evaluations of ecological niche models (ENMs, a.k.a. SDMs), which can estimate species' ranges and niche characteristics from data on species occurrences and environmental variables. 

Some of the most frequently used ENMs are machine learning algorithms with settings that can be "tuned" to determine optimal levels of model complexity. In implementation, this means building models of varying settings, then evaluating them and comparing their performance to select the optimal settings. Such tuning exercises can result in models that balance goodness-of-fit (i.e., avoiding overfitting) and predictive ability. Model evaluation is often done with cross validation, which consists of partitioning the data into groups, building a model with all the groups but one, evaluating this model on the left-out group, then repeating the process until all groups have been left out once.

The primary function, `ENMevaluate`, does all the heavy lifting and returns several items including a table of evaluation statistics and, for each setting combination (here, colloquially: *runs*), a model object and a raster layer showing the model prediction across the study extent. There are also options for calculating niche overlap between predictions, running in parallel to speed up computation, and more. For a more detailed description of the package, check out the open-access publication:

[Muscarella, R., Galante, P. J., Soley-Guardia, M., Boria, R. A., Kass, J. M., Uriarte, M. and Anderson, R. P. (2014), ENMeval: An R package for conducting spatially independent evaluations and estimating optimal model complexity for Maxent ecological niche models. Methods in Ecology and Evolution, 5: 1198â€“1205.](http://onlinelibrary.wiley.com/doi/10.1111/2041-210X.12261/full)

Older versions (v0.3.0 and earlier) implemented only [Maxent](http://biodiversityinformatics.amnh.org/open_source/maxent/) and [maxnet](https://cran.r-project.org/package=maxnet), but v1.0.0 and onward can implement any ENM in theory provided its settings are specified in a ENMdetails object (see below for an example). This version comes out of the box with Maxent, maxnet, [BIOCLIM](https://onlinelibrary.wiley.com/doi/full/10.1111/ddi.12144), and [BRT](https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/j.1365-2656.2008.01390.x) (boosted regression trees), but users can provide their own custom model specifications. 

## Data Acquisition & Pre-processing {#data}
In this vignette, we briefly demonstrate acquisition and pre-processing of input data for `ENMeval`. There are a number of other excellent tutorials on these steps, some of which we compiled in the [Resources](#resources) section.

We'll start by downloading an occurrence dataset for [*Bradypus variegatus*](https://en.wikipedia.org/wiki/Brown-throated_sloth), the Brown-throated sloth.  We'll go ahead and load the `ENMeval`, `dplyr` for data management, and [`spocc`](https://cran.r-project.org/package=spocc) packages (which we use to download occurrence records).

```{r occDownload}
library(spocc)
library(dplyr)
library(ENMeval)

# set a random seed for reproducibility
set.seed(48)

# Search GBIF for occurrence data.
bv <- occ('Bradypus variegatus', 'gbif', limit=300, has_coords=TRUE)

# Get the latitude/coordinates for each locality. Also convert the tibble that occ() outputs
# to a data frame for compatibility with ENMeval functions.
occs <- as.data.frame(bv$gbif$data$Bradypus_variegatus[,2:3])

# Remove duplicate rows (Note that you may or may not want to do this).
occs <- occs[!duplicated(occs),]
```

We are going to model the climatic niche suitability for our focal species using climate data from [WorldClim](http://www.worldclim.org/). WorldClim has a range of variables available at various resolutions; for simplicity, here we'll use the 9 bioclimatic variables at 10 arcmin resolution (about 20 km across at the equator) included in the `dismo` package. These climatic data are based on 50-year averages from 1950-2000. Now's also a good time to load the package, as it includes all the downstream dependencies (`raster`, `dismo`, etc.).

```{r envDownload, warning=FALSE, message=FALSE, fig.width = 5, fig.height = 5}
library(raster)

# Locate some predictor raster files from the dismo folder
files <- list.files(path=paste(system.file(package='dismo'), '/ex', sep=''), pattern='grd', full.names=TRUE)

# Read the raster files into a RasterStack
envs <- stack(files)

# Plot first raster in the stack, bio1
plot(envs[[1]], main=names(envs)[1])

# Add points for all the occurrence points onto the raster
points(occs)

# There are some points east on the Amazon. Let's say we know that this represents a subpopulation that we don't want to include in the model -- we can remove these points from the analysis by subsetting the occurrences by latitude and longitude
occs <- filter(occs, latitude > -20, longitude < -45)

# Plot the subsetted occurrences over the original ones to make sure we excluded the right ones
points(occs, col='red')

# As we will demonstrate model evaluation using independent data, we will specify a fake independent occurrence dataset
occs.ind <- data.frame(longitude = -runif(10, 55, 65), latitude = runif(10, 0, 5))
points(occs.ind, col="blue")
```

Next, we will specify the background extent by cropping (or "clipping" in ArcGIS terms) our global predictor variable rasters to a smaller region. Since our models will compare the environment at occurrence (i.e., presence) localities to the environment at background localities, we need to sample random points from a background extent. To help ensure we don't include areas that are suitable for our species but are unoccupied due to limitations like dispersal constraints, we will conservatively define the background extent as an area surrounding our occurrence localities. We will do this by buffering a bounding box that includes all occurrence localities. Some other methods of background extent delineation (e.g., minimum convex hulls) are more conservative because they better characterize the geographic space holding the points. In any case, this is one of the many things that you will need to carefully consider for your own study.

```{r backgExt, message=FALSE, warning=FALSE}
library(sf)
library(sp)

# Make a SpatialPoints object
occs.sf <- st_as_sf(occs, coords = c("longitude","latitude"), crs = 4326)
# occs.sf <- st_transform(occs.sf, "+proj=cea +lat_ts=0 +lon_0=-65.302734375")

# Buffer all occurrences by 5 degrees
# * this will give a warning because we are not buffering in a projected coordinate system
# * for simplicity, this vignette does not involve coordinate reference system (CRS) transformations,
# * but for a real analysis, transforming to a projected CRS before buffering is best practice
occs.buf <- st_buffer(occs.sf, dist = 5)
plot(envs[[1]], main = names(envs)[1])
plot(occs.buf, add = TRUE)

# Crop environmental rasters to match the study extent
envs.bg <- crop(envs, occs.buf)
# Next, mask the rasters to the shape of the buffers
envs.bg <- mask(envs.bg, occs.buf)
```

We may want to restrict the study extent to the continental areas (for example). We can use the simplified world map from the [`maptools`](https://cran.r-project.org/package=maptools) package, which is not automatically loaded with `ENMeval`, and tools from `sf` to exclude the Caribbean from our study extent.

```{r removeCaribbean, message=FALSE, fig.width = 5, fig.height = 5, warning = FALSE}
library(maptools)

# Get a simple world countries polygon
data(wrld_simpl)

# Get polygons for Central and South America
central.amer <- wrld_simpl@data$SUBREGION==5
south.amer <- wrld_simpl@data$SUBREGION==13
# Subset wrld_simpl and convert to sf
ca.sa <- st_as_sf(wrld_simpl[central.amer | south.amer,])

# Buffer by 1 degree before masking to make sure you don't clip the coastline
ca.sa.buf <- st_buffer(ca.sa, dist = 1)

# Mask envs by the combined Central-South America polygon excluding the Caribbean
envs.bg <- mask(envs.bg, ca.sa.buf)

# Plot one of the new masked rasters -- we still have a few cells from the Galapagos, but are now missing the Caribbean
plot(envs.bg[[1]], main=names(envs.bg)[1])
points(occs)
```

In the next step, we'll sample 10,000 random points from the background (note that the number of background points is also a consideration you should make with respect to your own study).

```{r backgPts, fig.width = 5, fig.height = 5}
library(dismo)

# Randomly sample 10,000 background points from one background extent raster (only one per cell without replacement). Note: Since the raster has <10,000 pixels, you'll get a warning and all pixels will be used for background. We will be sampling from the biome variable because it is missing some grid cells, and we are trying to avoid getting background points with NA.
bg <- randomPoints(envs.bg[[9]], n=10000)
bg <- as.data.frame(bg)
colnames(bg) <- colnames(occs)

# Notice how we have pretty good coverage (every cell).
plot(envs.bg[[1]], legend=FALSE)
points(bg, col='red')
```

## Partitioning Occurrences for Evaluation {#partition}
A run of ENMevaluate begins by using one of six methods to partition occurrence localities into testing and training bins (folds) for *k*-fold cross-validation (Fielding and Bell 1997; Peterson et al. 2011). Data partitioning is done internally by `ENMevaluate()`, but can also be done externally with the partitioning functions. In this section, we explain and illustrate these different functions.

1. [Spatial Block](#block)
2. [Spatial Checkerboard](#cb1)
3. [Spatial Hierarchical Checkerboard](#cb2)
4. [Jackknife (leave-one-out)](#jack)
5. [Random *k*-fold](#rand)
6. [Independent](#ind)
7. [User](#user)

The first three partitioning methods are variations of what Radosavljevic and Anderson (2014) referred to as 'masked geographically structured' data partitioning. Basically, these methods partition both occurrence records and background points into evaluation bins based on some spatial rules. The intention is to reduce spatial-autocorrelation between points that are included in the testing and training bins, which can overinflate model performance, at least for data sets that result from biased sampling (Veloz 2009; Hijmans 2012; Wenger and Olden 2012).

#### 1. Block {#block}
First, the 'block' method partitions data according to the latitude and longitude lines that divide the occurrence localities into four spatial groups of (insofar as possible) equal numbers. Both occurrence and background localities are assigned to each of the four bins based on their position with respect to these lines. The resulting object is a list of two vectors that supply the bin designation for each occurrence and background point.

```{r part.block, fig.width = 5, fig.height = 5}
block <- get.block(occs, bg)
plot.grps(pts = occs, pts.grp = block$occ.grp, envs = envs.bg)
# PLotting the background shows that the background extent is partitioned in a way that maximizes evenness
# of points across the four bins, not to maximize evenness of area
plot.grps(pts = bg, pts.grp = block$bg.grp, envs = envs.bg)
```

#### 2. Checkerboard1 {#cb1}
The next two partitioning methods are variants of a 'checkerboard' approach to partition occurrence localities. These generate checkerboard grids across the study extent and partition the localities into groups based on where they fall on the checkerboard. In contrast to the block method, both checkerboard methods subdivide geographic space equally but do not ensure a balanced number of occurrence localities in each bin. For these methods, the user needs to provide a raster layer on which to base the underlying checkerboard pattern. Here we simply use the predictor variable RasterStack. Additionally, the user needs to define an *aggregation.factor*. This value tells the number of grids cells to aggregate when making the underlying checkerboard pattern.

The Checkerboard1 method partitions the points into *k* = 2 spatial groups using a simple checkerboard pattern.

```{r part.ck1, fig.width = 5, fig.height = 5}
cb1 <- get.checkerboard1(occs, envs.bg, bg, aggregation.factor=5)
plot.grps(pts = occs, pts.grp = cb1$occ.grp, envs = envs.bg)
# Plotting the background points shows the checkerboard pattern very clearly
plot.grps(pts = bg, pts.grp = cb1$bg.grp, envs = envs.bg)

# We can increase the aggregation factor to give the groups bigger boxes
cb1.large <- get.checkerboard1(occs, envs.bg, bg, aggregation.factor=30)
plot.grps(pts = occs, pts.grp = cb1.large$occ.grp, envs = envs.bg)
plot.grps(pts = bg, pts.grp = cb1.large$bg.grp, envs = envs.bg)
```

#### 3. Checkerboard2 {#cb2}
The Checkerboard2 method partitions the data into *k* = 4 spatial groups. This is done by hierarchically aggregating the input raster at two scales. Presence and background groups are assigned based on which box they fall on the hierarchical checkerboard.

```{r part.ck2, fig.width = 5, fig.height = 5}
cb2 <- get.checkerboard2(occs, envs.bg, bg, aggregation.factor=c(5,5))
plot.grps(pts = occs, pts.grp = cb2$occ.grp, envs = envs.bg)
# Plotting the background points shows the checkerboard pattern very clearly
plot.grps(pts = bg, pts.grp = cb2$bg.grp, envs = envs.bg)
```

#### 4. Jackknife (leave-one-out) {#jack}
The next two methods differ from the first three in that (i) they do not partition the background points into different groups, and (ii) they do not account for spatial autocorrelation between testing and training localities. Primarily when working with relatively small data sets (e.g. < ca. 25 presence localities), users may choose a special case of *k*-fold cross-validation where the number of bins (*k*) is equal to the number of occurrence localities (*n*) in the data set (Pearson et al. 2007; Shcheglovitova and Anderson 2013). This is referred to as jackknife, or leave-one-out, partitioning.  As *n* models are processed with this partitioning method, the computation time could be long for large occurrence datasets. The background is not partitioned with jackknife.

```{r part.jk, fig.width = 5, fig.height = 5}
jack <- get.jackknife(occs, bg)
# If the number of input points is larger than 10, the legend for the groups is suppressed
plot.grps(pts = occs, pts.grp = jack$occ.grp, envs = envs.bg)
```

#### 5. Random k-fold {#rand}
The 'random k-fold' method partitions occurrence localities randomly into a user specified number of (*k*) bins. This method is equivalent to the 'cross-validate' partitioning scheme available in the current version of the Maxent software GUI. Especially with larger occurrence datasets, this partitioning method could randomly result in some spatial clustering of groups, which is why spatial partitioning methods are preferable for addressing spatial autocorrelation. Below, we partition the data into five random groups. The background is not partitioned with random k-fold.

```{r part.rand, fig.width = 5, fig.height = 5}
rand <- get.randomkfold(occs, bg, k = 5)
plot.grps(pts = occs, pts.grp = rand$occ.grp, envs = envs.bg)
```

#### 6. Independent {#ind}: 
The 'independent' method evaluates the model on an independent dataset that is not used to create the full model, and thus cross validation statistics are not calculated. To illustrate this, we will make a table containing occurrences representing both training data and independent testing data and plot the partitions in the same way as above. However, unlike before, the independent testing data (group 2) will not become training data for a new model. Instead, the training data (group 1) is used to make the model, and the independent testing data (group 2) is used only to evaluate it. Thus, the background extent does not include the independent testing data (a few points fall inside this extent because of the buffer we applied, but they are not used as training data).

```{r part.ind, fig.width = 5, fig.height = 5}
pts <- rbind(occs, occs.ind)
pts.grp <- c(rep(1, nrow(occs)), rep(2, nrow(occs.ind)))
plot.grps(pts = pts, pts.grp = pts.grp, envs = envs.bg)
```

#### 7. User-defined {#user}
For maximum flexibility, the last partitioning method is designed so that users can define *a priori* partitions. This provides a flexible way to conduct spatially-independent cross-validation with background masking. For example, we demonstrate partitioning the occurrence data based on *k*-means groups. The user-defined partition option can also be used to input partition groups derived from other sources, such as the [`blockCV`](https://github.com/rvalavi/blockCV) package available on Github.

```{r part.user1, fig.width = 5, fig.height = 5}
grp.n <- 6
kmeans <- kmeans(occs, grp.n)
occ.grp <- kmeans$cluster
plot.grps(pts = occs, pts.grp = occ.grp, envs = envs.bg)
```

When using the user-defined partitioning method, we need to supply ENMevaluate with group identifiers for both occurrence points AND background points. If we want to use all background points for each group, we can set the background to zero.

```{r part.user2, fig.width = 5, fig.height = 5}
bg.grp <- rep(0, nrow(bg))
plot.grps(pts = bg, pts.grp = bg.grp, envs = envs.bg)
```

Alternatively, we may think of various ways to partition background data. This depends on the goals of the study but we might, for example, find it reasonable to partition background by clustering around the centroids of the occurrence clusters.

```{r part.user3, fig.width = 5, fig.height = 5}
centers <- kmeans$center
d <- pointDistance(bg, centers, lonlat=T)
bg.grp <- apply(d, 1, function(x) which(x==min(x)))
plot.grps(pts = bg, pts.grp = bg.grp, envs = envs.bg)
```

Choosing among these data partitioning methods depends on the research objectives and the characteristics of the study system. Refer to the [Resources](Resources) section for additional considerations on appropriate partitioning for evaluation.

## Running ENMeval {#eval}
Once you decide which method of data partitioning you would like to use, you are ready to start building models. We now move on to the main function in ENMeval: `ENMevaluate`.

- [Initial considerations](#eval.consid)
- [Different parameterizations](#eval.parameterizations)
- [Exploring the results (the ENMevaluate object)](#eval.explore)

#### Initial considerations {#eval.consid}
The two main parameters to define when calling `ENMevaluate` are (1) the range of regularization multiplier values and (2) the combinations of feature class to consider. The ***regularization multiplier*** (RM) determines the penalty for adding parameters to the model. Higher RM values impose a stronger penalty on model complexity and thus result in simpler (*flatter*) model predictions. The ***feature classes*** determine the potential shape of the response curves. A model that is only allowed to include linear feature classes will most likely be simpler than a model that is allowed to include all possible feature classes. Much more description of these parameters is available in the [Resources](#resources) section. For the purposes of this vignette, we demonstrate simply how to adjust these parameters. The following section deals with comparing the outputs of each model.

Unless you supply the function with background points (which is recommended in many cases), you will need to define how many background points should be used with the 'n.bg' argument. If any of your predictor variables are categorical (e.g., biomes), you will need to define which layer(s) these are using the 'categoricals' argument.

ENMevaluate builds a separate model for each unique combination of RM values and feature class combinations. For example, the following call will build and evaluate 2 models. One with RM=1 and another with RM=2, both allowing only linear features.

<!-- ```{r load_vignette_data, echo = FALSE} -->
<!-- data(eval2) -->
<!-- ``` -->

```{r enmeval1, eval=FALSE}
maxnet1 <- ENMevaluate(occ = occs, env = envs, bg.coords = bg, mod.name = 'maxnet',
                             partitions = 'checkerboard2', tune.args = list(fc = "L", rm = 1:2))
```

We may, however, want to compare a wider range of models that can use a wider variety of feature classes and regularization multipliers:

```{r maxnet2, eval=TRUE}
maxnet2 <- ENMevaluate(occ = occs, env = envs, bg.coords = bg, mod.name = 'maxnet',
                             partitions = 'checkerboard2', tune.args = list(fc = c("L","LQ","LQP"), rm = 1:3))
```

When building many models, the command may take a long time to run. Of course this depends on the size of your dataset and the computer you are using. When working on big projects, running the command in parallel (`parallel=T`) can be faster. Note that running parallel can also be slower when working on small projects...

```{r maxnet2par, eval=FALSE}
maxnet2.par <- ENMevaluate(occ = occs, env = envs, bg.coords = bg, mod.name = 'maxnet',
                             partitions = 'checkerboard2', tune.args = list(fc = c("L","LQ","LQP"), rm = 1:3),
                             parallel = TRUE)
```

Another way to save time at this stage is to turn off the option that generates model predictions across the full study extent (`rasterPreds=F`). Note, however, that the full model predictions are needed for calculating AICc values so those are returned as NA in the results table when the `rasterPreds` argument is set to NULL.

```{r maxnet2.noRas, eval=FALSE}
maxnet2.noRas <- ENMevaluate(occ = occs, env = envs, bg.coords = bg, mod.name = 'maxnet',
                             partitions = 'checkerboard2', tune.args = list(fc = c("L","LQ","LQP"), rm = 1:3),
                             rasterPreds = NULL)
```

We can also calculate one of two niche overlap statistics while running `ENMevaluate` by setting the arguments `overlap=T` and `overlapStat`, which support Moran's I or Schoener's D. Note that you can also calculate this value at a later stage using the separate `calc.niche.overlap` function.

```{r maxnet2.overlap, results='hide'}
overlap <- calc.niche.overlap(maxnet2@predictions, overlapStat="D")
overlap
```

#### Different parameterizations {#eval.parameterizations}
There are multiple ways to run the function `ENMevaluate()`, and we will go over how to specify each parameterization and what the effects are on the results.

full.mod.pred: raster model prediction based on `envs`
cbi: a) calculated on raster model prediction based on `envs`, b) cbi.test cannot only be calculated for partitions "randomkfold", "independent", or "user"


```{r evalExamples, eval= F}
# This example is for Maxent models, and so we will specify the tune.tbl with ranges of feature classes and regularization multipliers
tune.args <- list(fc = c("L", "LQ", "H", "LQH"), rm = 1:5)

# 1. Standard: we are using the R package maxnet to avoid frequent user issues with rJava
eval <- ENMevaluate(occs, envs, bg, mod.name = "maxnet", 
                    tune.args = tune.args, categoricals = "biome", partitions = "block")
# If maxent.jar is installed and rJava loads properly, you can also run Maxent with the original Java software
eval.mxjar <- ENMevaluate(occs, envs, bg, mod.name = "maxent.jar", 
                    tune.args = tune.args, categoricals = "biome", partitions = "block")

# 2. Independent partition: no cross validation statistics calculated; instead, model will be evaluated on an independent dataset that is not used to create the full model
eval.indPart <- ENMevaluate(occs[1:250,], envs, bg, mod.name = "maxnet", 
                        tune.args = tune.args, categoricals = "biome", partitions = "independent", 
                        occs.ind = occs[251:nrow(occs),])

# 3. User partitions
user.grp <- list(occ.grp = round(runif(nrow(occs), 1, 2)), 
                 bg.grp = round(runif(nrow(bg), 1, 2)))
eval.userPart <- ENMevaluate(occs, envs, bg, mod.name = "maxnet", 
                         tune.args = tune.args, categoricals = "biome", partitions = "user", 
                         user.grp = user.grp)

# 4. No partitions: no cross validation statistics calculated, nor any model evaluation on test data
eval.noCV <- ENMevaluate(occs, envs, bg, mod.name = "maxnet", 
                         tune.args = tune.args, categoricals = "biome", partitions = "none")

# 5. No raster data (a.k.a, samples with data, or SWD): no full model raster predictions created, so will run faster; also, both cbi.train and cbi.test will be calculated on the point data (training and testing background) instead of on the "envs" rasters (default)
occs.vals <- cbind(occs, raster::extract(envs, occs))
bg.vals <- cbind(bg, raster::extract(envs, bg))
eval.swd <- ENMevaluate(occs.vals, bg = bg.vals, mod.name = "maxnet", 
                        tune.args = tune.args, categoricals = "biome", partitions = "block")

# 6. Background test CBI: this will calculate cbi.test on the point data (training and testing background) instead of on the "envs" rasters (default)
eval.bgCBI <- ENMevaluate(occs, envs, bg, mod.name = "maxnet", 
                          tune.args = tune.args, categoricals = "biome", partitions = "block", 
                          cbi.eval = "bg")

# 7. User model: models will be fit and evaluated with the user-specified model and associated settings 
mod1 <- ENMdetails(...)
eval.userENM <- ENMevaluate(occs, envs, bg, user.enm = mod1, 
                             tune.args = tune.args, categoricals = "biome", partitions = "block")
```

#### Exploring the results {#eval.explore}
Now let's take a look at the output from `ENMevaluate` (which is an object of class `ENMevaluation`) in more detail (also see `?ENMevaluation`).  It contains the following slots:

- `algorithm` A character vector showing which algorithm was used
- `tune.settings` A data.frame of settings that were tuned
- `partition.method` A character of partitioning method used
- `results` A data.frame of evaluation summary statistics
- `results.grp` A data.frame of evaluation k-fold statistics
- `models` A list of model objects
- `predictions` A RasterStack of model predictions
- `occ.pts` A data.frame of occurrence coordinates used for model training
- `occ.grp` A vector of partition groups for occurrence points
- `bg.pts` A data.frame of background coordinates used for model training
- `bg.grp` A vector of partition groups for background points
- `overlap` A list of matrices of pairwise niche overlap statistics

Let's first examine the structure of the object:
```{r results1}
eval

str(eval, max.level=3)
```

We will use helper functions to access the slots in the ENMevaluate object.
```{r algorithm}
# Access algorithm, tuning settings, and partition method information
algorithm(eval)
tune.settings(eval)
partition.method(eval)
# Results table with summary statistics for cross validation on test data
results(eval)
# Results table with cross validation statistics for each test partition
results.grp(eval)
# List of models with names corresponding to tune.args column label
models(eval)
# The "betas" slot in a maxnet model is a named vector of the variable coefficients 
# and what kind they are (in R formula notation)
# Note that the html file that is created when maxent.jar is run is **not** kept
m1 <- models(eval)[["LQH_1"]]
m1$betas
# For maxent.jar models, we can access this information in the lambdas slot
# The notation used here is difficult to decipher, so check out the [`rmaxent`](https://github.com/johnbaums/rmaxent/blob/master/) package
# available on Github for the `parse_lambdas()` function that makes it easier to read
m1.mxjar <- models(eval.mxjar)[["LQH_1"]]
m1.mxjar@lambdas
# We can also get a long list of results statistics from the results slot
m1.mxjar@results
# RasterStack of model predictions (for extent of "envs") with names corresponding 
# to tune.args column label
predictions(eval)
# Original occurrence data coordinates with associated predictor variable values
occs(eval)
# Background data coordinates with associated predictor variable values
bg(eval)
# Partition group assignments for occurrence data
occ.grp(eval)
# Partition group assignments for background data
bg.grp(eval)
```

## Plotting tuning results {#plot}
Plotting options in R are extremely flexible and here we demonstrate some key tools to explore the results of an ENMevaluate object graphically.

- [Plotting model predictions](#plot.preds)
- [Plotting response curves](#plot.resp)

ENMeval has a built-in ggplot-based plotting function (`eval.plot`) to visualize the results of the different models you tuned. Here, we will plot AICc and delta.AICc values for each feature class for both regularization multipliers.

```{r plot.res, fig.width = 5, fig.height = 5}
plot.eval(e = eval, stats = "or.mtp", col = "fc", x = "rm")
# We can plot more than one statistic at one with ggplot facetting
plot.eval(e = eval, stats = c("or.mtp", "auc.test"), col = "fc", x = "rm")
# Sometimes the error bars make it hard to visualize the plot, so we can try turning them off
plot.eval(e = eval, stats = c("or.mtp", "auc.test"), col = "fc", x = "rm", error.bars = FALSE)
# We can also fiddle with the dodge parameter to jitter the positions of overlapping points
plot.eval(e = eval, stats = c("or.mtp", "auc.test"), col = "fc", x = "rm", dodge = 0.5)
# Finally, we can switch which parameters are on the x-axis and which symbolized by color
# ENMeval currently only accepts two parameters for plotting at a time
plot.eval(e = eval, stats = c("or.mtp", "auc.test"), col = "rm", x = "fc", error.bars = FALSE)
```

#### Model selection {#eval.select}

Once we have our results, we will want to select one or more models that we think are optimal across all the models we ran. For this example, we will demonstrate how to select models using AICc (which does not consider cross-validation results) and a sequential method that selects models with the lowest average test omission rate, and to break ties, next with the highest average test AUC (this method uses cross-validation results).

```{r evaluations}
# Overall results
res <- results(eval)
opt.aicc <- res %>% filter(delta.AICc == 0)
opt.aicc
opt.seq <- res %>% 
  filter(or.mtp.avg == min(or.mtp.avg)) %>% 
  filter(auc.test.avg == max(auc.test.avg))
opt.seq
```

Let's now pull the optimal model (using the sequential criteria) from the model list and examine it.
```{r mod.obj1}
mod.seq <- models(eval)[[opt.seq$tune.args]]
# The non-zero coefficients in our model
mod.seq$betas
# The response curves for our model
# maxent.jar models use the dismo::response() funtion for this
plot(mod.seq)
dev.off()
```

Now let's pull out and plot the prediction raster for our optimal model. Note that by default for maxent.jar or maxnet models, these predictions are in the 'cloglog' output format that is bounded between 0 and 1 (see Phillips et al. 2017 for more details). This can be changed with the pred.type parameter in `ENMevaluate()`.
These predictions are for the entire extent of the input predictor variable rasters, and thus include areas outside of the background extent used for model training. Thus, we should interpret areas far outside this extent with caution.
```{r plotPred, fig.width = 5, fig.height = 5}
pred.seq <- predictions(eval)[[opt.seq$tune.args]]
plot(pred.seq)
# Let's also plot the binned background points with the occurrence points on top
points(bg(eval), pch = 3, col = bg.grp(eval), cex = 0.5)
points(occs(eval), pch = 21, bg = occ.grp(eval))
```

<!-- You can use the `var.importance` function to get a data.frame of two variable importance metrics: percent contribution and permutation importance.  See the function help file for more information on these metrics. -->
<!-- ```{r mod.obj4} -->
<!-- var.importance(aic.opt) -->
<!-- ``` -->

<!-- You can also plot the permutation importance or percent contribution. -->
<!-- ``` {r plot.res3, fig.width = 5, fig.height = 5} -->
<!-- df <- var.importance(aic.opt) -->
<!-- barplot(df$permutation.importance, names.arg=df$variable, las=2, ylab="Permutation Importance") -->
<!-- ``` -->

Let's see how model complexity changes the predictions in our example.  We'll compare the model built with only linear feature classes and the highest regularization multiplier value we used (i.e., fc='L', RM=5) with the model built with linear, quadratic, and hinge feature classes and the lowest regularization multiplier value we used (i.e., fc='LQH',  RM=1). We will first examine the response curves, and then the mapped model model predictions. Notice how the simpler models tend to have more smooth predictions of suitability, while the complex ones tend to show more patchiness. For more on simplicity and complexity in ENMs, see [Merow et al. 2014](https://onlinelibrary.wiley.com/doi/full/10.1111/ecog.00845).

```{r plot.pred2, fig.width = 5, fig.height = 2.5}
# First, let's examine the non-zero model coefficients in the betas slot
# The simpler model has fewer model coefficients
models(eval)[['L_5']]$betas
length(models(eval)[['L_5']]$betas)
models(eval)[['LQH_1']]$betas
length(models(eval)[['LQH_1']]$betas)
# Next, let's take a look at the response curves
# The complex model has responses with more curves (quadratic terms) and spikes (hinge terms)
plot(models(eval)[['L_5']])
plot(models(eval)[['LQH_1']])
# Finally, let's cut the plotting area into two rows to visualize the predictions side-by-side
par(mfrow=c(2,1), mar=c(2,1,2,0))
# The simplest model: linear features only and high regularization
plot(predictions(eval)[['L_5']], ylim=c(-30,20), xlim=c(-90,-30), legend=F, main='L_5 prediction')
# The most complex model: linear, quadratic, and hinge features with low regularization
plot(predictions(eval)[['LQH_1']], ylim=c(-30,20), xlim=c(-90,-30), legend=F, main='LQH_1 prediction')
```

## Downstream Analyses (*under construction*) {#downstream}
Below is a running list of other things we plan to add to this vignette.  Feel free to let us know if there are particular things you would like to see added.

- Working with the output from the 'maxnet' algorithm
- Extracting model results from object (various thresholds)
- Use model object to make a new prediction (e.g., if you want a logistic prediction)
- Make a projection to a new extent
- Do MESS map (Use `mess()` in the `dismo` package)

## Resources (*under construction*) {#resources}

###### Web Resources
- [Hijmans, R. and Elith, J. (2016) Species distribution modeling with R. dismo vignette.](https://cran.r-project.org/package=dismo)

- [Phillips, S. J. (2006) Phillips, S. (2006) A brief tutorial on Maxent. AT&T Research. Available at: http://biodiversityinformatics.amnh.org/open_source/maxent/](http://biodiversityinformatics.amnh.org/open_source/maxent/)

- [Yoder, J. (2013) Species distribution models in R. The Molecular Ecologist.](http://www.molecularecologist.com/2013/04/species-distribution-models-in-r/)

- [Maxent Google Group](https://groups.google.com/forum/embed/#!forum/maxent)

###### General Guides
- [Merow, C., Smith, M., and Silander, J.A. (2013) A practical guide to Maxent: what it does, and why inputs and settings matter. Ecography 36, 1-12.](http://onlinelibrary.wiley.com/doi/10.1111/j.1600-0587.2013.07872.x/abstract)

- [Peterson, A.T., SoberÃ³n, J., Pearson, R.G., Anderson, R.P., MartÃ­nez-Meyer, E., Nakamura, M., and AraÃºjo, M.B. (2011) Ecological Niches and Geographic Distributions. Monographs in Population Biology, 49. Princeton University Press.](http://press.princeton.edu/titles/9641.html)

- [Renner, I.W., Elith, J., Baddeley, A., Fithian, W., Hastie, T., Phillips, S.J., . . . Warton, D.I. (2015) Point process models for presence-only analysis. Methods in Ecology and Evolution 6, 366-379.](http://onlinelibrary.wiley.com/doi/10.1111/2041-210X.12352/abstract)

###### Model Evaluation
- [Aiello-Lammens, M.E., Boria, R.A., Radosavljevic, A., Vilela, B., and Anderson, R.P. (2015) spThin: an R package for spatial thinning of species occurrence records for use in ecological niche models. Ecography 38, 541-545.](http://onlinelibrary.wiley.com/doi/10.1111/ecog.01132/abstract)

- [Fielding, A.H. and Bell, J.F. (1997) A review of methods for the assessment of prediction errors in conservation presence-absence models. Environmental Conservation 24, 38-49.](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.463.359&rep=rep1&type=pdf)

- [Hijmans, R.J. (2012) Cross-validation of species distribution models: removing spatial sorting bias and calibration with a null model. Ecology 93, 679-688.](http://onlinelibrary.wiley.com/doi/10.1890/11-0826.1/abstract)

- [Muscarella, R., Galante, P. J., Soley-Guardia, M., Boria, R. A., Kass, J. M., Uriarte, M. and Anderson, R. P. (2014), ENMeval: An R package for conducting spatially independent evaluations and estimating optimal model complexity for Maxent ecological niche models. Methods Ecol Evol, 5: 1198â€“1205.](http://onlinelibrary.wiley.com/doi/10.1111/2041-210X.12261/full)

- [Radosavljevic, A. and Anderson, R.P. (2014) Making better Maxent models of species distributions: complexity, overfitting and evaluation. Journal of Biogeography 41, 629-643.](http://onlinelibrary.wiley.com/doi/10.1111/jbi.12227/abstract)

- [Shcheglovitova, M. and Anderson, R.P. (2013) Estimating optimal complexity for ecological niche models: A jackknife approach for species with small sample sizes. Ecol. Model. 269, 9-17.](http://www.sciencedirect.com/science/article/pii/S0304380013004043)

- [Veloz, S.D. (2009) Spatially autocorrelated sampling falsely inflates measures of accuracy for presence-only niche models. Journal of Biogeography 36, 2290-2299.](http://onlinelibrary.wiley.com/doi/10.1111/j.1365-2699.2009.02174.x/abstract)

- [Wenger, S.J. and Olden, J.D. (2012) Assessing transferability of ecological models: an underappreciated aspect of statistical validation. Methods in Ecology and Evolution 3, 260-267.](http://onlinelibrary.wiley.com/doi/10.1111/j.2041-210X.2011.00170.x/abstract)

###### Some Empirical Examples
- [Pearson, R.G., Raxworthy, C.J., Nakamura, M., and Peterson, A.T. (2007) Predicting species distributions from small numbers of occurrence records: a test case using cryptic geckos in Madagascar. Journal of Biogeography 34, 102-117.](http://onlinelibrary.wiley.com/doi/10.1111/j.1365-2699.2006.01594.x/abstract)


